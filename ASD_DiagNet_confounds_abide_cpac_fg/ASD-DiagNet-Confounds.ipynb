{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### This notebok implements ASD-DiagNet using the following features computed from the multiple linear regression and ComBat harmonization models, and normalization methods using static functional connectivity (sFC), computed from the ABIDE rs-FMRI preprocessed data, as dependent variables, and cc200 as the brain atlas:\n",
    "\n",
    "1. sFC: static functional connectivity (sfc_feature,file: sfc_feature_file_cc200.pkl ),\n",
    "2. $\\Delta$ mlrA: mlr residual of sFC with age as independent variable (sfc_mlr_age_feature, file: sfc_mlr_age_feature_file_cc200.pkl)\n",
    "3. $\\Delta$ mlrA$_{FZ}$: mlr residual of the Fisher Z-transform of sFC(sFC$_{FZ}$) with age as independent variable (sfc_fz_mlr_age_feature, file: sfc_fz_mlr_age_feature_file_cc200.pkl)\n",
    "4. $\\Delta$ mlrF: mlr residual of sFC with FIQ as independent variable (sfc_mlr_FIQ_feature, file: sfc_mlr_FIQ_feature_file_cc200.pkl)\n",
    "5. $\\Delta$ mlrF$_{FZ}$:mlr residual of the Fisher Z-transform of sFC(sFC$_{FZ}$) with FIQ as independent variable (sfc_fz_mlr_FIQ_feature, file: sfc_fz_mlr_FIQ_feature_file_cc200.pkl)\n",
    "6. $\\Delta$ mlrM: mlr residual of sFC with MRI vendor as independent variable (sfc_mlr_MRI_feature, file: sfc_mlr_MRI_feature_file_cc200.pkl)\n",
    "7. $\\Delta$ mlrM$_{FZ}$:mlr residual of the Fisher Z-transform of sFC(sFC$_{FZ}$) with MRI as independent variable (sfc_fz_mlr_MRI_feature, file: sfc_fz_mlr_MRI_feature_file_cc200.pkl)\n",
    "8. $\\Delta$ mlrG: mlr residual of sFC with gender as independent variable (sfc_mlr_gender_feature, file: sfc_mlr_MRI_feature_file_cc200.pkl)\n",
    "9. $\\Delta$ mlrG$_{FZ}$:mlr residual of the Fisher Z-transform of sFC(sFC$_{FZ}$) with gender as independent variable (sfc_fz_mlr_gender_feature, file: sfc_fz_mlr_gender_feature_file_cc200.pkl)\n",
    "10. $\\Delta$ mlrAGM: mlr residual of sFC with age,gender and MRI vendor as independent variables (sfc_mlr_AGM_feature, file: sfc_mlr_AGM_feature_file_cc200.pkl)\n",
    "11. $\\Delta$ mlrAGM$_{FZ}$:mlr residual of the Fisher Z-transform of sFC(sFC$_{FZ}$) with age,gender and MRI vendor as independent variables (sfc_fz_mlr_AGM_feature, file: sfc_fz_mlr_AGM_feature_file_cc200.pkl)\n",
    "12. cbA: ComBat harmonization of SFC with age as independent variable (sfc_combat_age_feature, file: sfc_combat_age_feature_file_cc200.pkl)\n",
    "13. cbA$_{FZ}$: ComBat harmonization of the Fisher Z-transform of sFC(sFC$_{FZ}$) with age as independent variable (sfc_fz_combat_age_feature, file: sfc_fz_combat_age_feature_file_cc200.pkl)\n",
    "14. cbF: ComBat harmonization of SFC with FIQ as independent variable (sfc_combat_FIQ_feature, file: sfc_combat_age_feature_file_cc200.pkl)\n",
    "15. cbF$_{FZ}$: ComBat harmonization of the Fisher Z-transform of sFC(sFC$_{FZ}$) with FIQ as independent variable (sfc_fz_combat_FIQ_feature, file: sfc_fz_combat_FIQ_feature_file_cc200.pkl)\n",
    "16. cbAFG: ComBat harmonization of SFC with age, FIQ and gender as independent variables (sfc_combat_AFG_feature, file: sfc_combat_AFG_feature_file_cc200.pkl)\n",
    "17. cbAFG$_{FZ}$: ComBat harmonization of the Fisher Z-transform of sFC(sFC$_{FZ}$) with age, FIQ and gender as independent variables (sfc_fz_combat_AFG_feature, file: sfc_fz_combat_AFG_feature_file_cc200.pkl)\n",
    "18. $\\Delta$ avg: demeaning of the  sFC with the average of sFC over all ABIDE subjects (sfc_res_avg_feature, file: sfc_res_avg_feature_file_cc200.pkl)\n",
    "19. $\\Delta$ avgSite: demeaning of the sFC on a given site  with the average of sFC over all ABIDE subjects for the given site (sfc_res_avg_site_feature, file: sfc_res_avg_site_feature_file_cc200.pkl)\n",
    "20. $\\Delta$ avgSubj: demeaning of the sFC with the average of sFC computed for each ABIDE subject  (sfc_res_avg_subj_feature, file: sfc_res_avg_subj_feature_file_cc200.pkl)\n",
    "\n",
    "\n",
    "### This notebook also implements ASD-DiagNet using baseline sub-samples of the ABIDE sites and homogeneous sub-samples of the ABIDE subjects. \n",
    "\n",
    "#### The baseline sub-samples were formed by progressively selecting the sites with the greatest values of accuracy computed with ASD-DiagNet for the classification of control and autistic subjects, with sFC features and the cc200 as the brain atlas. The baseline sub-samples are:\n",
    "\n",
    "bss_4 was formed with the 4 sites which obtained accuracies $\\ge$ 70.0: \n",
    "\n",
    "bss_4 = ['KKI','OHSU','Olin','USM'] \n",
    "\n",
    "bss_5 was formed with the 5 sites which obtained accuracies $\\ge$ 66.8: \n",
    "\n",
    "bss_5 = ['KKI','NYU','OHSU','Olin','USM'] \n",
    "\n",
    "bss_6 was formed with the 6 sites which obtained accuracies $\\ge$ 66.4:\n",
    "\n",
    "bss_6 =['KKI','NYU','OHSU','Olin','UCLA',\n",
    "        'USM'] \n",
    "\n",
    "bss_7 was formed with the 7 sites which obtained accuracies > 64.6: \n",
    "\n",
    "bss_7 =['KKI','NYU','OHSU','Olin','UCLA',\n",
    "        'USM','Yale'] \n",
    "\n",
    "bss_8 was formed with the 8 sites which obtained accuracies > 63.9: \n",
    "\n",
    "bss_8 =['KKI','NYU','OHSU','Olin','Stanford',\n",
    "        'UCLA','USM','Yale'] \n",
    "\n",
    "bss_9 was formed with the 9 sites which obtained accuracies > 63.8: \n",
    "\n",
    "bss_9 = ['CMU','KKI','NYU','OHSU','Olin',\n",
    "         'Stanford','UCLA','USM','Yale'] \n",
    "\n",
    "bss_10 was formed with the 10 sites which obtained accuracies > 63.4: \n",
    "\n",
    "bss_10 = ['CMU','KKI','NYU','OHSU','Olin',\n",
    "          'Stanford','UCLA','UM','USM','Yale']\n",
    " \n",
    "bss_11 was formed with the 11 sites which obtained accuracies > 62.4:\n",
    "\n",
    "bss_11 =['CMU','KKI','Leuven','NYU','OHSU',\n",
    "         'Olin','Stanford','UCLA','UM','USM',\n",
    "         'Yale'] \n",
    "\n",
    "bss_12 was formed with the 12 sites which obtained accuracies > 61.4: \n",
    "\n",
    "bss_12 =['CMU','KKI','Leuven','NYU','OHSU',\n",
    "         'Olin','Pitt','Stanford','UCLA','UM',\n",
    "         'USM','Yale'] \n",
    "\n",
    "bss_13 was formed with the 13 sites which obtained accuracies > 55.9: \n",
    "\n",
    "bss_13 =['CMU','KKI','Leuven','NYU','OHSU',\n",
    "         'Olin','Pitt','SDSU','Stanford','UCLA',\n",
    "         'UM','USM','Yale'] \n",
    "         \n",
    "bss_14 was formed with the 14 sites which obtained accuracies > 55.0: \n",
    "\n",
    "bss_14 =['CMU','KKI','Leuven','NYU','OHSU',\n",
    "         'Olin','Pitt','SBL','SDSU','Stanford',\n",
    "         'UCLA','UM','USM','Yale'] \n",
    "\n",
    "bss_15 was formed with the 15 sites which obtained accuracies > 54.0: \n",
    "\n",
    "bss_15 =['CMU','KKI','Leuven','MaxMun','NYU',\n",
    "         'OHSU','Olin','Pitt','SBL','SDSU',\n",
    "         'Stanford','UCLA','UM','USM','Yale'] \n",
    "\n",
    "bss_16 was formed with the 16 sites which obtained accuracies > 52.1: \n",
    "\n",
    "bss_16 =['Caltech','CMU','KKI','Leuven','MaxMun',\n",
    "         'NYU','OHSU','Olin','Pitt','SBL',\n",
    "          'SDSU','Stanford','UCLA','UM','USM',\n",
    "          'Yale'] \n",
    "\n",
    "whole designates all the 17 sites:\n",
    "\n",
    "whole = ['Caltech','CMU','KKI','Leuven','MaxMun',\n",
    "         'NYU','OHSU','Olin','Pitt','SBL',\n",
    "         'SDSU','Stanford','Trinity','UCLA','UM',\n",
    "          'USM','Yale']\n",
    "\n",
    "#### The homogeneous sub-samples of the ABIDE subjects were integrated by subjects classified by ranges of age, and ranges of FIQ. The  homogeneous sub-samples are:\n",
    "\n",
    "hss_age_10 was formed with 12  sites with subjects for which 0< age<=10: \n",
    "\n",
    "hss_age_10 = ['KKI','MaxMun','NYU','OHSU','Olin',\n",
    "              'Pitt','SDSU','Stanford','UCLA','UM',\n",
    "              'USM','Yale'] \n",
    "\n",
    "hss_age_1015 was formed with 14  sites with subjects for which 10 < age<=15: \n",
    "\n",
    "hss_age_1015 = ['KKI','Leuven','MaxMun','NYU','OHSU',\n",
    "                'Olin','Pitt','SDSU','Stanford','Trinity',\n",
    "                'UCLA','UM','USM','Yale'] \n",
    "\n",
    "hss_age_1520 was formed with 15 sites with subjects for which 15 < age<=20:\n",
    "\n",
    "hss_age_1520 = ['Caltech','CMU','Leuven','MaxMun','NYU',\n",
    "                'OHSU','Olin','Pitt','SBL','SDSU',\n",
    "                'Trinity','UCLA','UM','USM','Yale'] \n",
    "\n",
    "hss_age_1020 (age_1020) was formed with 17  sites with subjects for which 10 < age<=20:\n",
    "\n",
    "hss_age_1020 = ['Caltech','CMU','KKI','Leuven','MaxMun',\n",
    "                'NYU','OHSU','Olin','Pitt','SBL',\n",
    "                'SDSU','Stanford','Trinity','UCLA','UM',\n",
    "                'USM','Yale'] \n",
    "\n",
    "hss_age_20 was formed with 11  sites with subjects for which age>20:\n",
    "\n",
    "hss_age_20  = ['Caltech','CMU','Leuven','MaxMun','NYU',\n",
    "               'Olin','Pitt','SBL','Trinity','UM','USM'] \n",
    "               \n",
    "hss_FIQ_89 was formed with 14 sites with subjects for which 0 < FIQ <=89, p_fold = 5:\n",
    "\n",
    "hss_FIQ_89 = ['KKI','Leuven','MaxMun','NYU','OHSU',\n",
    "              'Olin','Pitt','SDSU','Stanford','Trinity',\n",
    "              'UCLA','UM','USM','Yale']\n",
    "\n",
    "hss_FIQ_89_110 was formed with 16 sites with subjects for which 89 < FIQ <=110:\n",
    "\n",
    "hss_FIQ_89_110 = ['Caltech','CMU','KKI','Leuven','MaxMun',\n",
    "                  'NYU','OHSU','Olin','Pitt','SDSU',\n",
    "                  'Stanford','Trinity','UCLA','UM','USM',\n",
    "                  'Yale'] \n",
    "\n",
    "hss_FIQ_110  was formed with 16 sites with subjects for which FIQ > 110:\n",
    "\n",
    "hss_FIQ_110 = ['Caltech','CMU','KKI','Leuven','MaxMun',\n",
    "               'NYU','OHSU','Olin','Pitt','SDSU',\n",
    "               'Stanford','Trinity','UCLA','UM','USM',\n",
    "               'Yale']\n",
    "\n",
    "hss_age_1020_FIQ_89110 was formed with 15 sites with subjects for which 89 < FIQ <=110 \n",
    "and 10<age<=20:\n",
    "\n",
    "hss_age_1020_FIQ_89110  = ['CMU','KKI','Leuven','MaxMun''NYU',\n",
    "                           'OHSU','Olin','Pitt','SDSU','Stanford',\n",
    "                           'Trinity','UCLA','UM','USM','Yale'] \n",
    "                           \n",
    "hss_age_1020_FIQ_89 was formed with 14 sites with subjects for which 0< FIQ <=89 and 10<age<=20,\n",
    "p_fold = 5:\n",
    "\n",
    "hss_age_1020_FIQ_89 = ['KKI','Leuven','MaxMun','NYU','OHSU',\n",
    "                       'Olin','Pitt','SDSU','Stanford','Trinity',\n",
    "                       'UCLA','UM','USM','Yale'] \n",
    "\n",
    "hss_FIQ_89_bal was formed with 14 sites with subjects for which 0<IQ<=89, plus a \n",
    "number of control subjects out of the sub-sample to balance it:\n",
    "\n",
    "hss_FIQ_89_bal = ['KKI','Leuven','MaxMun','NYU','OHSU',\n",
    "                  'Olin','Pitt','SDSU','Stanford','Trinity',\n",
    "                  'UCLA','UM','USM','Yale']\n",
    "                  \n",
    "hss_age_1020_FIQ_89_bal was formed with 14 sites with subjects for which 0< IQ <=89 and 10<age<=20,\n",
    "plus a number of control subjects out of the sub-sample to balance it:\n",
    "\n",
    "hss_age_1020_FIQ_89_bal  = ['KKI','Leuven','MaxMun','NYU','OHSU',\n",
    "                            'Olin','Pitt','SDSU','Stanford','Trinity',\n",
    "                            'UCLA','UM','USM','Yale']                   \n",
    "\n",
    "                          \n",
    "#### The starting parameters to run this notebook are defined as follows:\n",
    "\n",
    "1) Brain atlas: p_ROI = ['cc200', 'aal', 'dosenbach160','ez','ho','tt']. Notice that a brain atlas diferent that cc200 can be used, but the new features corresponding to the new brain atlas need to be computed using the sfcfeatures.py module.  \n",
    "\n",
    "2) feature key: p_feature = [‘sfc','sfc_mlr_age','sfc_fz_mlr_age','sfc_mlr_FIQ',\n",
    "                             'sfc_fz_mlr_FIQ','sfc_mlr_MRI','sfc_fz_mlr_MRI',\n",
    "                             'sfc_mlr_gender','sfc_fz_mlr_gender','sfc_mlr_AGM','sfc_fz_mlr_AGM',\n",
    "                             'sfc_combat_age','sfc_fz_combat_age','sfc_combat_FIQ',\n",
    "                             'sfc_fz_combat_FIQ','sfc_combat_AFG', 'sfc_fz_combat_AFG',\n",
    "                             'sfc_res_avg','sfc_res_avg_site','sfc_res_avg_subj']\n",
    "                             \n",
    "             \n",
    "\n",
    "                            \n",
    "3) p-fold cross-validation: p_fold = 5 for one site computation, \n",
    "                            p_fold = 10 for more than one site computation\n",
    "                            \n",
    "4) Define site or sub-sample with:\n",
    "\n",
    "p_center = ['Caltech','CMU','KKI','Leuven','MaxMun','NYU','OHSU', 'Olin',\n",
    "            'Pitt','SBL','SDSU','Stanford','UCLA','UM','USM','Yale','Trinity',\n",
    "            'bss_4','bss_5','bss_6','bss_7','bss_8','bss_9','bss_10','bss_11',\n",
    "            'bss_12','bss_13','bss_14','bss_15','bss_16','whole',\n",
    "            'hss_age_10','hss_age_1015','hss_age_1520','hss_age_1020',\n",
    "            'hss_age_20','hss_FIQ_89','hss_FIQ_89_110','hss_FIQ_110',\n",
    "            'hss_age_1020_FIQ_89110','hss_age_1020_FIQ_89','hss_FIQ_89_bal',\n",
    "            'hss_age_1020_FIQ_89_bal']\n",
    "            \n",
    "\n",
    " \n",
    "5) Classification mode as site or sub-sample: \n",
    "   p_mode = ['site', 'baseline sub-sample', 'homogeneous sub-sample']\n",
    "\n",
    "    Example: if p_center = 'Caltech', then p_mode = 'site',\n",
    "             if p_center = 'bss_4', then p_mode = 'baseline sub-sample', \n",
    "             if p_center = 'hss_age_10', then p_mode = 'baseline sub-sample'\n",
    "             \n",
    "6) Utilizing augmentation technique: p_augmentation = [False. True]\n",
    "\n",
    "7) Utilizing shuffle technique: p_shuffle = [False. True]\n",
    "\n",
    "8) p_max_add_control: maximum number of additional control subjects to balance an homogenous subsample.\n",
    "\n",
    "9) p_ss_length = subsamples_length[p_center]: subsample length\n",
    "\n",
    "### Classes implemented in the module asdiagnetconf.py:\n",
    "\n",
    "1. MultiSiteData\n",
    "\n",
    "2. SubSamples a subclass of the class MultiSiteData\n",
    "\n",
    "3. HelperFunctions a subclass of the class MultiSiteData\n",
    "\n",
    "4. MTAutoEncoder a subclass of the class nn.Module\n",
    "\n",
    "5. DiagDataLoader a subclass of the class HelperFunctions\n",
    "\n",
    "Repository:  https://github.com/pcdslab/ASD-DiagNet-Confounds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Possibility to stop warnings\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings('ignore') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "#import matplotlib.pyplot as plt\n",
    "import os\n",
    "from functools import reduce\n",
    "import time\n",
    "import torch.utils.tensorboard\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "import torch\n",
    "import pyprind\n",
    "import sys\n",
    "import pickle\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from scipy import stats\n",
    "import functools\n",
    "import numpy.ma as ma # for masked arrays\n",
    "import pyprind\n",
    "import random\n",
    "\n",
    "# sklearn library\n",
    "from sklearn.model_selection import KFold, StratifiedKFold\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "#asdiagnet module\n",
    "from asdiagnetconf import MultiSiteData as MSD\n",
    "from asdiagnetconf import SubSamples as SS\n",
    "from asdiagnetconf import HelperFunctions as HF\n",
    "from asdiagnetconf import MTAutoEncoder as MTA\n",
    "from asdiagnetconf import DiagDataLoader as DL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "center_list = ['Caltech','CMU','KKI','Leuven','MaxMun','NYU','OHSU', 'Olin', 'Pitt','SBL','SDSU','Stanford',\n",
    "            'Trinity','UCLA','UM','USM','Yale', 'bss_4','bss_5','bss_6','bss_7','bss_8','bss_9','bss_10',\n",
    "            'bss_11', 'bss_12','bss_13','bss_14','bss_15','bss_16','whole', 'hss_age_10','hss_age_1015',\n",
    "            'hss_age_1520','hss_age_1020', 'hss_age_20','hss_FIQ_89','hss_FIQ_89_110','hss_FIQ_110', \n",
    "            'hss_age_1020_FIQ_89110','hss_age_1020_FIQ_89','hss_FIQ_89_bal','hss_age_1020_FIQ_89_bal']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subsamples_length = {'bss_4':4,'bss_5':5,'bss_6':6,'bss_7':7,'bss_8':8,'bss_9':9,'bss_10':10,\n",
    "                  'bss_11':11,'bss_12':12,'bss_13':13,'bss_14':14,'bss_15':15,'bss_16':16,\n",
    "                  'whole':17,'hss_age_10':12,'hss_age_1015':14,'hss_age_1520':15,'hss_age_1020':17,\n",
    "                  'hss_age_20':11,'hss_FIQ_89':14,'hss_FIQ_89_110':17,'hss_FIQ_110':17, \n",
    "                  'hss_age_1020_FIQ_89110':17,'hss_age_1020_FIQ_89':14,'hss_FIQ_89_bal':14,\n",
    "                   'hss_age_1020_FIQ_89_bal':14}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_list = ['sfc','sfc_mlr_age','sfc_fz_mlr_age','sfc_mlr_FIQ', 'sfc_fz_mlr_FIQ','sfc_mlr_MRI',\n",
    "             'sfc_fz_mlr_MRI', 'sfc_mlr_gender','sfc_fz_mlr_gender','sfc_mlr_AGM','sfc_fz_mlr_AGM', \n",
    "             'sfc_combat_age','sfc_fz_combat_age','sfc_combat_FIQ', 'sfc_fz_combat_FIQ','sfc_combat_AFG', \n",
    "             'sfc_fz_combat_AFG', 'sfc_res_avg','sfc_res_avg_site','sfc_res_avg_subj']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mode_list = ['site', 'baseline sub-sample', 'homogeneous sub-sample']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Starting parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "tags": [
     "parameters"
    ]
   },
   "outputs": [],
   "source": [
    "\n",
    "p_ROI = 'cc200'\n",
    "p_feature =  feature_list[0] \n",
    "p_center  =  center_list[0] \n",
    "p_mode    =  mode_list[0]\n",
    "if p_mode ==  'site':\n",
    "    p_fold = 5\n",
    "    p_shuffle = True\n",
    "else: \n",
    "    p_fold = 10\n",
    "    p_shuffle = True\n",
    "p_augmentation = True\n",
    "if p_center == 'hss_FIQ_89' or p_center == 'hss_age_1020_FIQ_89':\n",
    "    p_fold = 2\n",
    "p_max_add_control = 0        \n",
    "if p_center == 'hss_FIQ_89_bal':\n",
    "        p_max_add_control += 34\n",
    "elif p_center == 'hss_age_1020_FIQ_89_bal':\n",
    "        p_max_add_control += 28       \n",
    "if p_mode ==  'baseline sub-sample' or p_mode ==  'homogeneous sub-sample' :\n",
    "    p_ss_length = subsamples_length[p_center]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "print(\"*****List of parameters****\")\n",
    "print('Brain atlas:' ,p_ROI)\n",
    "print('p_feature: ',p_feature)\n",
    "print('p_center: ',p_center)\n",
    "print('p_mode: ',p_mode)\n",
    "if p_mode == 'site':\n",
    "    print('Site: ',p_center)\n",
    "else:\n",
    "    print('Sub-sample: ',p_center)\n",
    "    print('Sub-sample length: ',p_ss_length)\n",
    "print('Augmentation:',p_augmentation)\n",
    "print('Shuffle:',p_shuffle)\n",
    "print('p_fold: ',p_fold)\n",
    "print('p_max_add_control: ',p_max_add_control)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# data_files_path: path to ABIDE data, input_data_path: path to features data,\n",
    "# data_phenotypic_path: path to phenotypic file\n",
    "# Please update data_path\n",
    "data_path = '~/abide_fmri_preprocessed/'\n",
    "data_phenotypic_path = data_path+'Phenotypic_V1_0b_preprocessed1.csv'\n",
    "data_files_path = data_path+ 'rois_'+p_ROI+'/'\n",
    "input_data_path = data_path+ 'rois_'+p_ROI+'_input/'\n",
    "print('data_phenotypic_path ',data_phenotypic_path)\n",
    "print('data_files_path', data_files_path)\n",
    "print('input_data_path', input_data_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Downloading feature and eigenvalue data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "f =open(input_data_path+p_feature+'_feature_file_'+p_ROI+'.pkl', 'rb')\n",
    "feat_data = pickle.load(f)\n",
    "f.close\n",
    "print('file for feature data downloaded:',p_feature+'_feature_file_'+p_ROI+'.pkl')\n",
    "f = open(input_data_path+'eig_data_'+p_feature+'_feature_file_'+p_ROI+'.pkl', 'rb')\n",
    "eig_data = pickle.load(f)\n",
    "f.close\n",
    "print('file for eig data downloaded: ','eig_data_',p_feature+'_feature_file_'+p_ROI+'.pkl')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Instances of the MultiSiteData (MSD), the HelperFunctions(HF), and  DataLoader (DL) classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "msd = MSD(data_phenotypic_path,data_files_path)\n",
    "hf = HF(data_phenotypic_path,data_files_path,feat_data,eig_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute subjects ids and phenotypic data\n",
    "msd.get_subjects_id()\n",
    "subjects_id = msd.subjects_id\n",
    "msd.get_phenotypic_data()\n",
    "labels = msd.labels\n",
    "#compute number of features\n",
    "num_feat = len(feat_data[subjects_id[0]][0])\n",
    "print('num_feat:',num_feat)\n",
    "#length of eig_data\n",
    "num_dim = len(eig_data[subjects_id[0]]['eigvals'])\n",
    "print('num_dim:',num_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dl = DL(data_phenotypic_path,data_files_path,feat_data,eig_data,num_dim)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Computing subsamples and centers_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if p_mode == 'baseline sub-sample' or p_mode == 'site':\n",
    "    hf.get_centers_dict()\n",
    "    centers_dict =  hf.centers_dict\n",
    "if p_mode == 'homogeneous sub-sample' or p_mode == 'baseline sub-sample':\n",
    "    ss = SS(data_phenotypic_path,data_files_path,p_center,p_max_add_control,p_ss_length)\n",
    "    if p_mode == 'homogeneous sub-sample':\n",
    "        ss.get_hss() \n",
    "        subsample = np.array (ss.subsample)\n",
    "        centers_dict = ss.centers_dict\n",
    "    elif  p_mode == 'baseline sub-sample':\n",
    "        ss.get_bss() \n",
    "        subsample = np.array (ss.subsample)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining training functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def train(model, epoch, train_loader, p_bernoulli=None, mode='both',lam_factor=1.0):         \n",
    "    model.train()\n",
    "    train_losses = []\n",
    "    for i,(batch_x,batch_y) in enumerate(train_loader):\n",
    "        if len(batch_x) != batch_size:\n",
    "            continue\n",
    "        if p_bernoulli is not None:\n",
    "            if i == 0:\n",
    "                p_tensor = torch.ones_like(batch_x).to(device)*p_bernoulli\n",
    "            rand_bernoulli = torch.bernoulli(p_tensor).to(device)\n",
    "\n",
    "        data, target = batch_x.to(device), batch_y.to(device)\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        if mode in ['both', 'ae']:\n",
    "            if p_bernoulli is not None:\n",
    "                rec_noisy, _ = model(data*rand_bernoulli, False)\n",
    "                loss_ae = criterion_ae(rec_noisy, data) / len(batch_x)\n",
    "            else:\n",
    "                rec, _ = model(data, False)\n",
    "                loss_ae = criterion_ae(rec, data) / len(batch_x)\n",
    "\n",
    "        if mode in ['both', 'clf']:\n",
    "            rec_clean, logits = model(data, True)\n",
    "            loss_clf = criterion_clf(logits, target)\n",
    "\n",
    "        if mode == 'both':\n",
    "            loss_total = loss_ae + lam_factor*loss_clf\n",
    "            train_losses.append([loss_ae.detach().cpu().numpy(), \n",
    "                                 loss_clf.detach().cpu().numpy()])\n",
    "        elif mode == 'ae':\n",
    "            loss_total = loss_ae\n",
    "            train_losses.append([loss_ae.detach().cpu().numpy(), \n",
    "                                 0.0])\n",
    "        elif mode == 'clf':\n",
    "            loss_total = loss_clf\n",
    "            train_losses.append([0.0, \n",
    "                                 loss_clf.detach().cpu().numpy()])\n",
    "\n",
    "        loss_total.backward()\n",
    "        optimizer.step()\n",
    "    #print('train_losses',train_losses)\n",
    "    return train_losses       "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Defining ASD-DiagNet parameters and similarity function (sim_function) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 50\n",
    "batch_size = 8\n",
    "learning_rate_ae, learning_rate_clf = 0.0001, 0.0001    \n",
    "p_bernoulli = None\n",
    "use_dropout = True    \n",
    "augmentation = p_augmentation\n",
    "aug_factor = 2\n",
    "num_neighbs = 5\n",
    "lim4sim = 2 \n",
    "sim_function = functools.partial(hf.cal_similarity, lim=lim4sim)\n",
    "run = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Computing classification scores with ASD-DiagNet for ABIDE sites, or for a homogeneous sub-sample, or for a baseline sub-sample. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print ('p_center:',p_center)\n",
    "print ('p_ROI: ', p_ROI )\n",
    "print ('p_feature: ',p_feature)\n",
    "print('num_epochs: ', num_epochs)\n",
    "print ('p_fold: ',p_fold)\n",
    "print ('shuffle: ',p_shuffle)\n",
    "print('augmentation: ', augmentation, 'aug_factor: ', aug_factor, \n",
    "          'num_neighbs: ', num_neighbs, 'lim4sim: ', lim4sim,\n",
    "          'number of features: ',num_feat)\n",
    "    \n",
    "if  p_mode == 'site':           \n",
    "    subj_id_list = np.array(centers_dict[p_center])\n",
    "            \n",
    "    output_name = 'results_site_diag.csv'    \n",
    "    results = open(output_name, 'a')\n",
    "    print('Result will written in {0}'.format(output_name))    \n",
    "    results.write('##########################################################################\\n'+\n",
    "                   'feature: '+str(p_feature) + '-p_center: '+str(p_center) + \n",
    "                    '-p_shuffle: '+str(p_shuffle) +\n",
    "                          ','+'Ac'+','+'Se'+','+'Sp'+'\\n')\n",
    "    results.close()\n",
    "    run = True\n",
    "    \n",
    "elif p_mode == 'homogeneous sub-sample':    \n",
    "    print ('homogeneous subsample ',p_center,':',subsample)            \n",
    "    length = subsample.shape[0]\n",
    "    print('length subsample:', length)\n",
    "    hf.get_subj_id_list(length,subsample,centers_dict)\n",
    "    subj_id_list = np.array(hf.subj_id_list)\n",
    "            \n",
    "    output_name = 'results_hss_diag.csv'    \n",
    "    results = open(output_name, 'a')    \n",
    "    print('Result will written in {0}'.format(output_name)) \n",
    "    results.write('##########################################################################\\n'+\n",
    "                   'feature: '+str(p_feature) + '-p_center: '+str(p_center) + \n",
    "                    '-p_shuffle: '+str(p_shuffle) +\n",
    "                          ','+'Ac'+','+'Se'+','+'Sp'+'\\n')\n",
    "    results.close()\n",
    "    run = True\n",
    "    \n",
    "elif  p_mode == 'baseline sub-sample':\n",
    "    print ('baseline subsample ',p_center,':',subsample)       \n",
    "    length = subsample.shape[0]\n",
    "    print('length subsample:', length)\n",
    "    hf.get_subj_id_list(length,subsample,centers_dict)\n",
    "    subj_id_list = np.array(hf.subj_id_list)\n",
    "            \n",
    "    output_name = 'results_bss_diag.csv'    \n",
    "    results = open(output_name, 'a')    \n",
    "    print('Result will written in {0}'.format(output_name)) \n",
    "    results.write('##########################################################################\\n'+\n",
    "                   'feature: '+str(p_feature) + '-p_center: '+str(p_center) + \n",
    "                    '-p_shuffle: '+str(p_shuffle) +\n",
    "                          ','+'Ac'+','+'Se'+','+'Sp'+'\\n')\n",
    "    results.close()\n",
    "    run = True\n",
    "    \n",
    "#compute the classification scores    \n",
    "if run: \n",
    "    hf.get_number_subjects(subj_id_list)\n",
    "    num_control_subj, num_autism_subj = hf.control_autism\n",
    "    num_subjects = len(subj_id_list)\n",
    "    print( 'subjects:',num_subjects, ',control:',num_control_subj,\n",
    "          ',autism:',num_autism_subj)    \n",
    "    labels_list = np.array([labels[subj] for subj in subj_id_list])\n",
    "    \n",
    "    all_rp_res=[]\n",
    "    kk=0 \n",
    "    repeat = 10   \n",
    "    total_time = 0    \n",
    "    start_time =time.time()\n",
    "    pbar = pyprind.ProgBar(repeat)\n",
    "    for rp in range(repeat): \n",
    "        crossval_res_kol=[]\n",
    "        if p_shuffle:\n",
    "            kf = StratifiedKFold(n_splits=p_fold, random_state=1, shuffle=True)\n",
    "            np.random.shuffle(subj_id_list)\n",
    "            labels_list = np.array([labels[subj] for subj in subj_id_list])\n",
    "        else:\n",
    "            kf = StratifiedKFold(n_splits=p_fold)    \n",
    "        for kk,(train_index, test_index) in enumerate(kf.split(subj_id_list, labels_list)):\n",
    "            train_samples, test_samples = subj_id_list[train_index], subj_id_list[test_index]\n",
    "            verbose = (True if (kk == 0) else False)\n",
    "            thr_regs = 0.25\n",
    "            reg_num =int(num_feat*thr_regs)\n",
    "            hf.get_regs_inds(train_samples,reg_num)\n",
    "            regions_inds = np.array(hf.regions_inds)\n",
    "            num_inpp = len(regions_inds)\n",
    "            lat_mult = 0.5\n",
    "            n_lat = int(num_inpp*lat_mult)\n",
    "            if kk == 0 and rp == 0:\n",
    "                print('thr_regs,lat_mult,num_inpp,n_lat: ',thr_regs,lat_mult,num_inpp,n_lat)\n",
    "            train_loader=dl.get_loader(data=feat_data, samples_list=train_samples, \n",
    "                                    batch_size=batch_size, mode='train',\n",
    "                                    augmentation=augmentation, aug_factor=aug_factor, \n",
    "                                    num_neighbs=num_neighbs,eig_data=eig_data, \n",
    "                                    similarity_fn=sim_function, \n",
    "                                    verbose=verbose,regions=regions_inds)                                    \n",
    "            test_loader=dl.get_loader(data=feat_data, samples_list=test_samples, \n",
    "                                   batch_size=batch_size, mode='test', augmentation=False, \n",
    "                                   verbose=verbose,regions=regions_inds)\n",
    "            model = MTA(tied=True, num_inputs=num_inpp, num_latent=n_lat, use_dropout=use_dropout)\n",
    "            model.to(device)\n",
    "            criterion_ae = nn.MSELoss(reduction='sum') #MSE: Mean Square Error\n",
    "            criterion_clf = nn.BCEWithLogitsLoss()     #BCE: Binary Cross Entropy\n",
    "            optimizer = optim.SGD([{'params': model.fc_encoder.parameters(), 'lr': learning_rate_ae},\n",
    "                                   {'params': model.classifier.parameters(), 'lr': learning_rate_clf}],\n",
    "                                    momentum=0.9)\n",
    "            for epoch in range(1, num_epochs+1):\n",
    "                if epoch <= 20:\n",
    "                    train_losses = train(model, epoch, train_loader, p_bernoulli, mode='both')\n",
    "                else:\n",
    "                    train_losses = train(model, epoch, train_loader, p_bernoulli, mode='clf')            \n",
    "            res_mlp = hf.test(model, criterion_ae, test_loader, \n",
    "                              eval_classifier=True,device=device)\n",
    "            print(res_mlp)\n",
    "            crossval_res_kol.append(res_mlp)\n",
    "       \n",
    "        r = np.mean(np.array(crossval_res_kol),axis = 0)\n",
    "        all_rp_res.append(r) \n",
    "                             \n",
    "        hf.output_repeat_results(rp,r,start_time)        \n",
    "        pbar.update() \n",
    "        \n",
    "    hf.output_results(all_rp_res,repeat,output_name,\n",
    "                      p_center,p_mode,p_ROI,p_feature)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Tags",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
